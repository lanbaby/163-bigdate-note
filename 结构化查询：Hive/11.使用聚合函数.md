## Hive函数分类

- 内置函数：hive自带的函数

    - 简单函数：一进一出，用于格式化、数据运算等

    - **聚合函数**：多进一出，用于统计计算、指标聚合等

    - 集合函数：array、map操作

    - 特殊函数：窗口函数等

- 自定义函数

## 聚合函数

- 商品详情页被访问了多少次？

    这里不关心是哪些用户(user_id)在什么时间(timt_tag)访问了什么内容(req_url)，这里比较关心的是有多少用户访问了a商品，有多少用户访问了b商品。

    明细数据包含的内容比较全面和细致，有时候我们不需要所有的信息，而是需要在详细信息的基础上抽象统计出一些信息，也就是汇总级别的统计数据，这就需要借助聚合函数来完成。

    使用sql来统计商品详情页面被访问了多少次：

    ```sql
    select
        req_url,
        count(1)
    from 
        bigdata.weblog
    where
        active_name='pageview'
        and
        req_url like '%/product/%'
    group by
        req_url
    ```
    
    执行结果：

    ```
    hive> select
        >     req_url,
        >     count(1)
        > from
        >     bigdata.weblog
        > where
        >     active_name='pageview'
        >     and
        >     req_url like '%/product/%'
        > group by
        >     req_url;
    Query ID = 1015146591_20190106191531_04f6fefb-c200-4e5c-8289-d3e0ceef75ed
    Total jobs = 1
    Launching Job 1 out of 1
    Number of reduce tasks not specified. Estimated from input data size: 7
    In order to change the average load for a reducer (in bytes):
    set hive.exec.reducers.bytes.per.reducer=<number>
    In order to limit the maximum number of reducers:
    set hive.exec.reducers.max=<number>
    In order to set a constant number of reducers:
    set mapreduce.job.reduces=<number>
    Starting Job = job_1535253853575_21371, Tracking URL = http://bigdata0.novalocal:8                                                                                                         088/proxy/application_1535253853575_21371/
    Kill Command = /home/hadoop/hadoop-current/bin/hadoop job  -kill job_1535253853575                                                                                                         _21371
    Hadoop job information for Stage-1: number of mappers: 6; number of reducers: 7
    2019-01-06 19:15:38,750 Stage-1 map = 0%,  reduce = 0%
    2019-01-06 19:15:46,196 Stage-1 map = 17%,  reduce = 0%, Cumulative CPU 6.28 sec
    2019-01-06 19:15:49,354 Stage-1 map = 20%,  reduce = 0%, Cumulative CPU 47.61 sec
    2019-01-06 19:15:51,450 Stage-1 map = 26%,  reduce = 0%, Cumulative CPU 60.83 sec
    2019-01-06 19:15:52,491 Stage-1 map = 30%,  reduce = 0%, Cumulative CPU 70.48 sec
    2019-01-06 19:15:53,548 Stage-1 map = 36%,  reduce = 0%, Cumulative CPU 73.61 sec
    2019-01-06 19:15:54,577 Stage-1 map = 48%,  reduce = 0%, Cumulative CPU 87.11 sec
    2019-01-06 19:15:56,644 Stage-1 map = 59%,  reduce = 2%, Cumulative CPU 93.23 sec
    2019-01-06 19:15:57,674 Stage-1 map = 70%,  reduce = 4%, Cumulative CPU 105.71 sec
    2019-01-06 19:15:58,710 Stage-1 map = 90%,  reduce = 7%, Cumulative CPU 108.24 sec
    2019-01-06 19:15:59,749 Stage-1 map = 100%,  reduce = 12%, Cumulative CPU 110.22 s                                                                                                         ec
    2019-01-06 19:16:00,783 Stage-1 map = 100%,  reduce = 56%, Cumulative CPU 117.64 s                                                                                                         ec
    2019-01-06 19:16:01,809 Stage-1 map = 100%,  reduce = 100%, Cumulative CPU 127.27                                                                                                          sec
    MapReduce Total cumulative CPU time: 2 minutes 7 seconds 270 msec
    Ended Job = job_1535253853575_21371
    MapReduce Jobs Launched:
    Stage-Stage-1: Map: 6  Reduce: 7   Cumulative CPU: 127.27 sec   HDFS Read: 1571453635 HDFS Write: 5800 SUCCESS
    Total MapReduce CPU Time Spent: 2 minutes 7 seconds 270 msec
    OK
    http://www.bigdataclass.com/product/1527235438746948    6582
    http://www.bigdataclass.com/product/1527235438747116    6646
    http://www.bigdataclass.com/product/1527235438747270    6510
    http://www.bigdataclass.com/product/1527235438748068    6562
    http://www.bigdataclass.com/product/1527235438748572    6495
    http://www.bigdataclass.com/product/1527235438748621    6535
    ……
    http://www.bigdataclass.com/product/1527235438748795    6649
    http://www.bigdataclass.com/product/1527235438749509    6521
    http://www.bigdataclass.com/product/1527235438749523    6618
    http://www.bigdataclass.com/product/1527235438749712    6476
    http://www.bigdataclass.com/product/1527235438749908    6478
    http://www.bigdataclass.com/product/1527235438750419    6698
    http://www.bigdataclass.com/product/1527235438750902    6535
    Time taken: 32.661 seconds, Fetched: 100 row(s)
    ```

- 男女用户谁更会花钱？

    统计男女用户哪一类人群下单金额更多，sql语句如下：

    ```sql
    select
        gender,
        count(t1.user_id) as count_order,
        sum(pay_amount) as sum_amount,
        avg(pay_amount) as avg_amount
    from
        (select user_id, pay_amount from bigdata.orders) t1
    join
        (select user_id, gender from bigdata.member) t2
    on t1.user_id=t2.user_id
    group by
        gender
    ```

    建立订单表：

    ```sql
    create external table `bigdata.orders` (
        `order_id`  string  comment '订单id',
        `user_id`   string  comment '用户id',
        `product_id`    string  comment '商品id',
        `order_time`  bigint  comment '下单时间',
        `pay_amount`    double  comment '金额'
    )
    row format SERDE
        'org.openx.data.jsonserde.JsonSerDe'
    STORED AS INPUTFORMAT
        'org.apache.hadoop.mapred.TextInputFormat'
    OUTPUTFORMAT
        'org.apache.hadoop.hive.ql.io.HiveIgnoreKeyTextOutputFormat'
    LOCATION
        '/user/hadoop/hive/orders'
    ```

    创建结果：

    ```
    hive> create external table `bigdata.orders` (
        >     `order_id`  string  comment '订单id',
        >     `user_id`   string  comment '用户id',
        >     `product_id`    string  comment '商品id',
        >     `order_time`  bigint  comment '下单时间',
        >     `pay_amount`    double  comment '金额'
        > )
        > row format SERDE
        >     'org.openx.data.jsonserde.JsonSerDe'
        > STORED AS INPUTFORMAT
        >     'org.apache.hadoop.mapred.TextInputFormat'
        > OUTPUTFORMAT
        >     'org.apache.hadoop.hive.ql.io.HiveIgnoreKeyTextOutputFormat'
        > LOCATION
        >     '/user/hadoop/hive/orders';
    OK
    Time taken: 0.076 seconds

    hive> desc orders;
    OK
    order_id                string                  订单id
    user_id                 string                  用户id
    product_id              string                  商品id
    order_time              bigint                  下单时间
    pay_amount              double                  金额
    Time taken: 0.34 seconds, Fetched: 5 row(s)
    ```

    统计男女用户哪一类人群下单金额更多执行结果：

    ```
    hive>     select
        >         gender,
        >         count(t1.user_id) as count_order,
        >         sum(pay_amount) as sum_amount,
        >         avg(pay_amount) as avg_amount
        >     from
        >         (select user_id, pay_amount from bigdata.orders) t1
        >     join
        >         (select user_id, gender from bigdata.member) t2
        >     on t1.user_id=t2.user_id
        >     group by
        >         gender;
    Query ID = 1015146591_20190106193552_f6a32384-3cdd-4343-a22a-5394c7a8b072
    Total jobs = 1
    Execution log at: /tmp/1015146591/1015146591_20190106193552_f6a32384-3cdd-4343-a22a-5394c7a8b072.log
    2019-01-06 19:35:56     Starting to launch local task to process map join;      maximum memory = 508559360
    2019-01-06 19:35:58     Dump the side-table for tag: 0 with group count: 15104 into file: file:/mnt/home/1015146591/apps/hive-1.2.2/tmp/1015146591/4195d5ac-cd0c-484a-ad5a-8a74a78d0bf6/hive_2019-01-06_19-35-52_975_5737141907165469961-1/-local-10004/HashTable-Stage-2/MapJoin-mapfile00--.hashtable
    2019-01-06 19:35:58     Uploaded 1 File to: file:/mnt/home/1015146591/apps/hive-1.2.2/tmp/1015146591/4195d5ac-cd0c-484a-ad5a-8a74a78d0bf6/hive_2019-01-06_19-35-52_975_5737141907165469961-1/-local-10004/HashTable-Stage-2/MapJoin-mapfile00--.hashtable (652905 bytes)
    2019-01-06 19:35:58     End of local task; Time Taken: 1.317 sec.
    Execution completed successfully
    MapredLocal task succeeded
    Launching Job 1 out of 1
    Number of reduce tasks not specified. Estimated from input data size: 1
    In order to change the average load for a reducer (in bytes):
    set hive.exec.reducers.bytes.per.reducer=<number>
    In order to limit the maximum number of reducers:
    set hive.exec.reducers.max=<number>
    In order to set a constant number of reducers:
    set mapreduce.job.reduces=<number>
    Starting Job = job_1535253853575_21374, Tracking URL = http://bigdata0.novalocal:8088/proxy/application_1535253853575_21374/
    Kill Command = /home/hadoop/hadoop-current/bin/hadoop job  -kill job_1535253853575_21374
    Hadoop job information for Stage-2: number of mappers: 1; number of reducers: 1
    2019-01-06 19:36:06,133 Stage-2 map = 0%,  reduce = 0%
    2019-01-06 19:36:12,393 Stage-2 map = 100%,  reduce = 0%, Cumulative CPU 4.96 sec
    2019-01-06 19:36:18,670 Stage-2 map = 100%,  reduce = 100%, Cumulative CPU 7.37 sec
    MapReduce Total cumulative CPU time: 7 seconds 370 msec
    Ended Job = job_1535253853575_21374
    MapReduce Jobs Launched:
    Stage-Stage-2: Map: 1  Reduce: 1   Cumulative CPU: 7.37 sec   HDFS Read: 4829952 HDFS Write: 78 SUCCESS
    Total MapReduce CPU Time Spent: 7 seconds 370 msec
    OK
    女      7635    474593.0        62.16018336607728
    男      7469    464502.0        62.19065470611862
    Time taken: 26.789 seconds, Fetched: 2 row(s)
    ```
