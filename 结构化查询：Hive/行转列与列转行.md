## 行转列业务场景

业务场景：用户分析

要做用户分析，但是原始数据中及记录的是每条日志详情，内容如下：

| 用户id | 购买商品id |
|:-----:|:---------:|
| 001 | Product1 |
| 001 | Product1 |
| 001 | Product1 |
| 001 | Product1 |
| 001 | Product1 |
| 001 | Product1 |
| ..... | ..... |

这个粒度太细，不便于我们分析，我们其实想得到的可能是用户维度的数据，类似下面这种：

| 用户id | 购买商品id |
|:-----:|:---------:|
| 001 | Product1, Product2, product3 |
| 002 | Product1, Product2, product3 |
| ... | ... |

也就是把它聚合成每个用户为一行，把一个用户购买的所有商品都聚合到一个字段中，这其实就是行转列的操作。

## Hive中行转列函数

>collect_set()： 不插入重复记录

> collect_list(): 保留重复记录

SQL测试语句:

```sql
select
    user_id,
    collect_set(product_id),
    collect_list(product_id)
from
    bigdata.orders
group by
    user_id
limit 10;
```

测试结果：

```
hive> select
    >     user_id,
    >     collect_set(product_id),
    >     collect_list(product_id)
    > from
    >     bigdata.orders
    > group by
    >     user_id
    > limit 10;
Query ID = 1015146591_20190107204725_cc30fd60-41fa-4137-b3cf-3cb5889586d8
Total jobs = 1
Launching Job 1 out of 1
Number of reduce tasks not specified. Estimated from input data size: 1
In order to change the average load for a reducer (in bytes):
  set hive.exec.reducers.bytes.per.reducer=<number>
In order to limit the maximum number of reducers:
  set hive.exec.reducers.max=<number>
In order to set a constant number of reducers:
  set mapreduce.job.reduces=<number>
Starting Job = job_1535253853575_21559, Tracking URL = http://bigdata0.novalocal:8088/proxy/application_1535253853575_21559/
Kill Command = /home/hadoop/hadoop-current/bin/hadoop job  -kill job_1535253853575_21559
Hadoop job information for Stage-1: number of mappers: 1; number of reducers: 1
2019-01-07 20:47:34,327 Stage-1 map = 0%,  reduce = 0%
2019-01-07 20:47:40,615 Stage-1 map = 100%,  reduce = 0%, Cumulative CPU 3.71 sec
2019-01-07 20:47:45,798 Stage-1 map = 100%,  reduce = 100%, Cumulative CPU 6.32 sec
MapReduce Total cumulative CPU time: 6 seconds 320 msec
Ended Job = job_1535253853575_21559
MapReduce Jobs Launched:
Stage-Stage-1: Map: 1  Reduce: 1   Cumulative CPU: 6.32 sec   HDFS Read: 2064643 HDFS Write: 510 SUCCESS
Total MapReduce CPU Time Spent: 6 seconds 320 msec
OK
0001528166031761        ["1527235438747744"]    ["1527235438747744"]
0001531897281376        ["1527235438746891"]    ["1527235438746891"]
0001531897292909        ["1527235438750711"]    ["1527235438750711"]
0001531897299559        ["1527235438750618"]    ["1527235438750618"]
0001531897444689        ["1527235438750087"]    ["1527235438750087"]
0001531898180216        ["1527235438750087"]    ["1527235438750087"]
0001531898269760        ["1527235438747610"]    ["1527235438747610"]
0001531898480773        ["1527235438748507"]    ["1527235438748507"]
0011528165930044        ["1527235438748829"]    ["1527235438748829"]
0011528165983913        ["1527235438746782"]    ["1527235438746782"]
Time taken: 21.048 seconds, Fetched: 10 row(s)
```

使用这两个行专列的函数类似于下表所示：

| 用户id | 购买商品id(collect_set) | 购买商品id(collect_list) |
|:------:|:----------------------:|:-----------------------:|
| 0001 | product1, product2 | product1, product2, product2 |
| 0002 | product1, product2, product3 | product1, product2, product3 |
| ... | ... | ... |

collect_set和collect_list就是聚合函数的一种，这里单独列出来讲是因为在实际业务中经常使用行转列和列转行这两种方法。

## 用户画像

用户画像是指根据用户的社会属性、生活习惯、消费行为等等各种信息抽象出来的一个标签化的用户模型，用户画像的作用是让用户的运营更加精细化，根据用户特点去做不同的运营措施。

![用户画像][1]

用户画像的核心就是对用户打标签，比如可以从业务数据中整理出下列用户信息：

- 性别

- 年龄

- 设备类型

- 注册时间

- 首次下单时间

- 最近一次下单时间

- 下单次数

- 下单金额

- 最近一次下单地理位置

- ...

根据这一系列数据就可以建立起一个庞大的用户画像标签库，基于这个标签库可以分析用户相关的各种行为，从而针对不同的用户指定不同的运营策略。

现在有一个问题：建立这些标签的数据往往是分布在不同的日志数据源中的，应该通过怎样的一种方式来建立用户画像标签库呢？

### 计算N个指标的实现方式一

通过一个个子语句从不同的数据表中加工出需要的数据指标，然后通过庞大的sql把这些指标串联起来，这是最直接的方式，往往也是最麻烦的方式。

sql语句大概是这个样子：

```sql
select
    user_id,
    gender,
    age,
    device_type,
    ...
from
    (select ... from ...) t1
left outer join
    (select ... from ...) t2
on t1.user_id=t2.user_id
left outer join
    (select ... from ...) t3
on t1.user_id=t2.user_id
......
```

这种方法的优点：直观

缺点：

- 代码冗长

- 字段输出顺序固定，可维护性弱

- 运行效率低下：所有指标都集中在一个sql语句中，不能并行执行，导致效率低下。

### 计算N个指标的实现方式二

使用行转列或列转行的方式来完成用户标签库的建立，把一步实现的方式分多步完成，一般步骤如下：

- 建立key-value中间表

- 分组计算各字段，列转行存入中间表

- 中间表行转列为大宽表



1. 先建一个中间表和一个大宽表：

    中间表：

    ```sql
    create external table `bigdata.user_tag_value` (
        `user_id`   string      comment     '用户组',
        `tag`       string      comment     '标签',
        `value`     string      comment     '标签值')
    partitioned by (
        `module`    string      comment     '标签模块')
    ROW FORMAT SERDE
        'org.openx.data.jsonserde.JsonSerDe'
    STORED AS INPUTFORMAT
        'org.apache.hadoop.mapred.TextInputFormat'
    OUTPUTFORMAT
        'org.apache.hadoop.hive.ql.io.HiveIgnoreKeyTextOutputFormat'
    location
        '/user/1015146591/hive/user_profile/tag_value'
    ```

    大宽表：

    ```sql
    create external table `bigdata.user_profile` (
        `user_id`   string      comment     '用户id',
        `profile`   string      comment     '用户标签')
    row format serde
        'org.openx.data.jsonserde.JsonSerDe'
    stored as inputformat
        'org.apache.hadoop.mapred.TextInputFormat'
    outputformat
        'org.apache.hadoop.hive.ql.io.HiveIgnoreKeyTextOutputFormat'
    location
        '/user/1015146591/hive/user_profile/user_profile'
    ```

    分别执行：

    ```
    hive>
        >
        > create external table `bigdata.user_tag_value` (
        >     `user_id`   string      comment     '用户组',
        >     `tag`       string      comment     '标签',
        >     `value`     string      comment     '标签值')
        > partitioned by (
        >     `module`    string      comment     '标签模块')
        > ROW FORMAT SERDE
        >     'org.openx.data.jsonserde.JsonSerDe'
        > STORED AS INPUTFORMAT
        >     'org.apache.hadoop.mapred.TextInputFormat'
        > OUTPUTFORMAT
        >     'org.apache.hadoop.hive.ql.io.HiveIgnoreKeyTextOutputFormat'
        > location
        >     '/user/1015146591/hive/user_profile/tag_value';
    OK
    Time taken: 0.195 seconds
    hive> create external table `bigdata.user_profile` (
        >     `user_id`   string      comment     '用户id',
        >     `profile`   string      comment     '用户标签')
        > row format serde
        >     'org.openx.data.jsonserde.JsonSerDe'
        > stored as inputformat
        >     'org.apache.hadoop.mapred.TextInputFormat'
        > outputformat
        >     'org.apache.hadoop.hive.ql.io.HiveIgnoreKeyTextOutputFormat'
        > location
        >     '/user/1015146591/hive/user_profile/user_profile';
    OK
    Time taken: 0.041 seconds
    ```

2. 然后计算各个字段，存入中间表，先分组group1，将用户的性别、年龄、设备类型、注册日期提取出来存放到中间表中：

    ```sql
    insert overwrite table bigdata.user_tag_value partition(module='basic_info')
    select
        user_id,
        mp['key'],
        mp['value']
    from
        (select
            user_id,
            array(
            map('key', 'gender', 'value', gender),
            map('key', 'age', 'value', cast(2018-from_unixtime(cast(birthday/1000 as bigint), 'yyyy') as string)),
            map('key', 'device_type', 'value', device_type),
            map('key', 'register_day', 'value', from_unixtime(cast(register_time/1000 as bigint), 'yyyy-MM-dd'))
            ) as arr
        from 
            bigdata.member
        ) s lateral view explode(arr) arrtable as mp
    ```

    执行结果：

    ```
    hive> insert overwrite table bigdata.user_tag_value partition(module='basic_info')
        > select
        >     user_id,
        >     mp['key'],
        >     mp['value']
        > from
        > (select
        >     user_id,
        >     array(map('key', 'gender', 'value', gender),
        >     map('key', 'age', 'value', cast(2018-from_unixtime(cast(birthday/1000 as bigint)                                                                                                                                                , 'yyyy') as string)),
        >     map('key', 'device_type', 'value', device_type),
        >     map('key', 'register_day', 'value', from_unixtime(cast(register_time/1000 as big                                                                                                                                                int), 'yyyy-MM-dd'))
        >     ) as arr
        > from
        >     bigdata.member) s lateral view explode(arr) arrtable as mp;
    Query ID = 1015146591_20190107223740_dc7872a5-d395-4673-815d-1db820668c96
    Total jobs = 3
    Launching Job 1 out of 3
    Number of reduce tasks is set to 0 since there's no reduce operator
    Starting Job = job_1535253853575_21572, Tracking URL = http://bigdata0.novalocal:8088/prox                                                                                                                                                y/application_1535253853575_21572/
    Kill Command = /home/hadoop/hadoop-current/bin/hadoop job  -kill job_1535253853575_21572
    Hadoop job information for Stage-1: number of mappers: 1; number of reducers: 0
    2019-01-07 22:37:46,508 Stage-1 map = 0%,  reduce = 0%
    2019-01-07 22:37:53,799 Stage-1 map = 100%,  reduce = 0%, Cumulative CPU 7.17 sec
    MapReduce Total cumulative CPU time: 7 seconds 170 msec
    Ended Job = job_1535253853575_21572
    Stage-4 is selected by condition resolver.
    Stage-3 is filtered out by condition resolver.
    Stage-5 is filtered out by condition resolver.
    Moving data to: hdfs://bigdata0:50000/user/1015146591/hive/user_profile/tag_value/module=basic_info/.hive-staging_hive_2019-01-07_22-37-40_449_2087025953513902676-1/-ext-10000
    Loading data to table bigdata.user_tag_value partition (module=basic_info)
    Partition bigdata.user_tag_value{module=basic_info} stats: [numFiles=1, numRows=120716, totalSize=7796231, rawDataSize=0]
    MapReduce Jobs Launched:
    Stage-Stage-1: Map: 1   Cumulative CPU: 7.17 sec   HDFS Read: 4822507 HDFS Write: 7796331 SUCCESS
    Total MapReduce CPU Time Spent: 7 seconds 170 msec
    OK
    Time taken: 14.745 seconds
    ```

    查看一下执行后中间表内的数据：

    ```sql
    select * from bigdata.user_tag_value limit 10;
    ```

    ```
    hive> select * from bigdata.user_tag_value limit 10;
    OK
    0001528166058390        gender  女      basic_info
    0001528166058390        age     26.0    basic_info
    0001528166058390        device_type     WEB     basic_info
    0001528166058390        register_day    2018-04-09      basic_info
    0001531897045258        gender  男      basic_info
    0001531897045258        age     34.0    basic_info
    0001531897045258        device_type     IPhone  basic_info
    0001531897045258        register_day    2018-03-26      basic_info
    0001531897084097        gender  女      basic_info
    0001531897084097        age     39.0    basic_info
    Time taken: 0.101 seconds, Fetched: 10 row(s)
    ```

3. 提取首次下单时间、最近下单时间、下单次数、下单金额

    ```sql
    insert overwrite table bigdata.user_tag_value partition(module='consume_info')
    select
        user_id,
        mp['key'],
        mp['value']
    from
        (select
            user_id,
            array(
            map('key', 'first_order_time', 'value', min(order_time)),
            map('key', 'last_order_time', 'value', max(order_time)),
            map('key', 'order_count', 'value', count(1)),
            map('key', 'order_sum', 'value', sum(pay_amount))
            ) as arr
        from 
            bigdata.orders
        group by
            user_id
        ) s lateral view explode(arr) arrtable as mp
    ```

    执行结果：

    ```
    hive> insert overwrite table bigdata.user_tag_value partition(module='consume_info')
        > select
        >     user_id,
        >     mp['key'],
        >     mp['value']
        > from
        >     (select
        >         user_id,
        >         array(
        >         map('key', 'first_order_time', 'value', min(order_time)),
        >         map('key', 'last_order_time', 'value', max(order_time)),
        >         map('key', 'order_count', 'value', count(1)),
        >         map('key', 'order_sum', 'value', sum(pay_amount))
        >         ) as arr
        >     from
        >         bigdata.orders
        >     group by
        >         user_id
        >     ) s lateral view explode(arr) arrtable as mp;
    Query ID = 1015146591_20190107225440_13303ef4-cfc3-4eae-bb89-42f04e10689e
    Total jobs = 1
    Launching Job 1 out of 1
    Number of reduce tasks not specified. Estimated from input data size: 1
    In order to change the average load for a reducer (in bytes):
    set hive.exec.reducers.bytes.per.reducer=<number>
    In order to limit the maximum number of reducers:
    set hive.exec.reducers.max=<number>
    In order to set a constant number of reducers:
    set mapreduce.job.reduces=<number>
    Starting Job = job_1535253853575_21573, Tracking URL = http://bigdata0.novalocal:8088/proxy/application_1535253853575_21573/
    Kill Command = /home/hadoop/hadoop-current/bin/hadoop job  -kill job_1535253853575_21573
    Hadoop job information for Stage-1: number of mappers: 1; number of reducers: 1
    2019-01-07 22:54:47,927 Stage-1 map = 0%,  reduce = 0%
    2019-01-07 22:54:53,307 Stage-1 map = 100%,  reduce = 0%, Cumulative CPU 3.77 sec
    2019-01-07 22:54:59,521 Stage-1 map = 100%,  reduce = 100%, Cumulative CPU 9.63 sec
    MapReduce Total cumulative CPU time: 9 seconds 630 msec
    Ended Job = job_1535253853575_21573
    Loading data to table bigdata.user_tag_value partition (module=consume_info)
    Partition bigdata.user_tag_value{module=consume_info} stats: [numFiles=1, numRows=60416, totalSize=4321419, rawDataSize=0]
    MapReduce Jobs Launched:
    Stage-Stage-1: Map: 1  Reduce: 1   Cumulative CPU: 9.63 sec   HDFS Read: 2068108 HDFS Write: 4321520 SUCCESS
    Total MapReduce CPU Time Spent: 9 seconds 630 msec
    OK
    Time taken: 19.868 seconds
    ```

4. 最后一组中间表指标，包含地理位置信息

    ```sql
    insert overwrite table bigdata.user_tag_value partition(module='geography_info')
    select
        user_id,
        'province' as key,
        province
    from
        (select
            user_id,
            row_number() over (partition by user_id order by time_tag desc) as order_rank,
            address['province'] as province
        from 
            bigdata.weblog
        where
            active_name = 'pay') t1
    where
        order_rank = 1
    ```

    执行结果如下：

    ```
    hive> insert overwrite table bigdata.user_tag_value partition(module='geography_info')
        > select
        >     user_id,
        >     'province' as key,
        >     province
        > from
        >     (select
        >         user_id,
        >         row_number() over (partition by user_id order by time_tag desc) as order_rank,
        >         address['province'] as province
        >     from
        >         bigdata.weblog
        >     where
        >         active_name = 'pay') t1
        > where
        >     order_rank = 1;
    Query ID = 1015146591_20190107230037_ea3bdeb8-6a0b-4111-bb4c-9df76751b914
    Total jobs = 1
    Launching Job 1 out of 1
    Number of reduce tasks not specified. Estimated from input data size: 7
    In order to change the average load for a reducer (in bytes):
    set hive.exec.reducers.bytes.per.reducer=<number>
    In order to limit the maximum number of reducers:
    set hive.exec.reducers.max=<number>
    In order to set a constant number of reducers:
    set mapreduce.job.reduces=<number>
    Starting Job = job_1535253853575_21574, Tracking URL = http://bigdata0.novalocal:8088/proxy/application_1535253853575_21574/
    Kill Command = /home/hadoop/hadoop-current/bin/hadoop job  -kill job_1535253853575_21574
    Hadoop job information for Stage-1: number of mappers: 6; number of reducers: 7
    2019-01-07 23:00:43,782 Stage-1 map = 0%,  reduce = 0%
    2019-01-07 23:00:52,071 Stage-1 map = 17%,  reduce = 0%, Cumulative CPU 6.24 sec
    2019-01-07 23:00:54,146 Stage-1 map = 18%,  reduce = 0%, Cumulative CPU 17.19 sec
    2019-01-07 23:00:55,190 Stage-1 map = 20%,  reduce = 0%, Cumulative CPU 60.04 sec
    2019-01-07 23:00:57,268 Stage-1 map = 26%,  reduce = 0%, Cumulative CPU 66.89 sec
    2019-01-07 23:00:58,329 Stage-1 map = 40%,  reduce = 0%, Cumulative CPU 77.07 sec
    2019-01-07 23:01:00,383 Stage-1 map = 48%,  reduce = 0%, Cumulative CPU 90.85 sec
    2019-01-07 23:01:02,481 Stage-1 map = 70%,  reduce = 2%, Cumulative CPU 98.15 sec
    2019-01-07 23:01:03,592 Stage-1 map = 80%,  reduce = 7%, Cumulative CPU 108.43 sec
    2019-01-07 23:01:04,632 Stage-1 map = 100%,  reduce = 10%, Cumulative CPU 110.95 sec
    2019-01-07 23:01:05,662 Stage-1 map = 100%,  reduce = 19%, Cumulative CPU 111.22 sec
    2019-01-07 23:01:06,701 Stage-1 map = 100%,  reduce = 43%, Cumulative CPU 117.57 sec
    2019-01-07 23:01:07,727 Stage-1 map = 100%,  reduce = 100%, Cumulative CPU 143.3 sec
    MapReduce Total cumulative CPU time: 2 minutes 23 seconds 300 msec
    Ended Job = job_1535253853575_21574
    Loading data to table bigdata.user_tag_value partition (module=geography_info)
    Partition bigdata.user_tag_value{module=geography_info} stats: [numFiles=7, numRows=40460, totalSize=2632564, rawDataSize=0]
    MapReduce Jobs Launched:
    Stage-Stage-1: Map: 6  Reduce: 7   Cumulative CPU: 143.3 sec   HDFS Read: 1571466260 HDFS Write: 2633278 SUCCESS
    Total MapReduce CPU Time Spent: 2 minutes 23 seconds 300 msec
    OK
    Time taken: 33.794 seconds

    ```

5. 最后一步，将中间表通过行专列，将其转换成一个大宽表

    ```sql
    insert overwrite table bigdata.user_profile
    select
        user_id,
        concat('{', concat_ws(',', collect_set(concat('"', tag, '"', ':', '"', value, '"'))), '}') as json_string
    from
        bigdata.user_tag_value
    group by
        user_id;
    ```

    执行结果：

    ```
    hive> insert overwrite table bigdata.user_profile
        > select
        >     user_id,
        >     concat('{', concat_ws(',', collect_set(concat('"', tag, '"', ':', '"', value, '"'))), '}') as json_string
        > from
        >     bigdata.user_tag_value
        > group by
        >     user_id;
    Query ID = 1015146591_20190107230714_4f1ca497-377e-4c69-9500-23d0640447f8
    Total jobs = 1
    Launching Job 1 out of 1
    Number of reduce tasks not specified. Estimated from input data size: 1
    In order to change the average load for a reducer (in bytes):
    set hive.exec.reducers.bytes.per.reducer=<number>
    In order to limit the maximum number of reducers:
    set hive.exec.reducers.max=<number>
    In order to set a constant number of reducers:
    set mapreduce.job.reduces=<number>
    Starting Job = job_1535253853575_21593, Tracking URL = http://bigdata0.novalocal:8088/proxy/application_1535253853575_21593/
    Kill Command = /home/hadoop/hadoop-current/bin/hadoop job  -kill job_1535253853575_21593
    Hadoop job information for Stage-1: number of mappers: 2; number of reducers: 1
    2019-01-07 23:07:44,411 Stage-1 map = 0%,  reduce = 0%
    2019-01-07 23:07:51,719 Stage-1 map = 50%,  reduce = 0%, Cumulative CPU 3.13 sec
    2019-01-07 23:07:55,855 Stage-1 map = 100%,  reduce = 0%, Cumulative CPU 13.74 sec
    2019-01-07 23:08:02,102 Stage-1 map = 100%,  reduce = 100%, Cumulative CPU 22.6 sec
    MapReduce Total cumulative CPU time: 22 seconds 600 msec
    Ended Job = job_1535253853575_21593
    Loading data to table bigdata.user_profile
    Table bigdata.user_profile stats: [numFiles=0, numRows=58424, totalSize=0, rawDataSize=0]
    MapReduce Jobs Launched:
    Stage-Stage-1: Map: 2  Reduce: 1   Cumulative CPU: 22.6 sec   HDFS Read: 14766922 HDFS Write: 8294101 SUCCESS
    Total MapReduce CPU Time Spent: 22 seconds 600 msec
    OK
    Time taken: 49.459 seconds

    ```

6. 查看一下最终得到的大宽表是怎样的结构：

    ```
    select * from bigdata.user_profile limit 10;
    ```

    执行结果：

    ```
    hive> select * from bigdata.user_profile limit 10;
    OK
    0001528165978481        {"gender":"女","age":"43.0","device_type":"Android","register_day":"2018-05-29"}
    0001528166031761        {"gender":"女","age":"44.0","device_type":"WEB","register_day":"2018-04-25","first_order_time":"1527578321393","last_order_time":"1527578321393","order_count":"1","order_sum":"40.0","province":"山西"}
    0001528166058390        {"gender":"女","age":"26.0","device_type":"WEB","register_day":"2018-04-09"}
    0001528166157577        {"gender":"男","age":"27.0","device_type":"WEB","register_day":"2018-01-10"}
    0001531897045258        {"gender":"男","age":"34.0","device_type":"IPhone","register_day":"2018-03-26"}
    0001531897084097        {"gender":"女","age":"39.0","device_type":"WEB","register_day":"2018-05-30"}
    0001531897098930        {"gender":"女","age":"21.0","device_type":"IPhone","register_day":"2018-02-13"}
    0001531897281376        {"gender":"男","age":"21.0","device_type":"IPhone","register_day":"2018-05-30","first_order_time":"1527642284658","last_order_time":"1527642284658","order_count":"1","order_sum":"64.0"}
    0001531897292909        {"gender":"男","age":"40.0","device_type":"WEB","register_day":"2018-03-03","first_order_time":"1527639077523","last_order_time":"1527639077523","order_count":"1","order_sum":"25.0"}
    0001531897299559        {"gender":"女","age":"35.0","device_type":"IPhone","register_day":"2018-01-07","first_order_time":"1527637397062","last_order_time":"1527637397062","order_count":"1","order_sum":"86.0"}
    Time taken: 0.037 seconds, Fetched: 10 row(s)
    ```

7. 使用大宽表举例：

    ```sql
    select 
        user_id,
        get_json_object(profile, '$.gender'),
        get_json_object(profile, '$.age'),
        get_json_object(profile, '$.province')
    from
        bigdata.user_profile
    limit 10;
    ```

    执行结果：

    ```
    hive> select
        >     user_id,
        >     get_json_object(profile, '$.gender'),
        >     get_json_object(profile, '$.age'),
        >     get_json_object(profile, '$.province')
        > from
        >     bigdata.user_profile
        > limit 10;
    OK
    0001528165978481        女      43.0    NULL
    0001528166031761        女      44.0    山西
    0001528166058390        女      26.0    NULL
    0001528166157577        男      27.0    NULL
    0001531897045258        男      34.0    NULL
    0001531897084097        女      39.0    NULL
    0001531897098930        女      21.0    NULL
    0001531897281376        男      21.0    NULL
    0001531897292909        男      40.0    NULL
    0001531897299559        女      35.0    NULL
    Time taken: 0.224 seconds, Fetched: 10 row(s)
    ```

### 总结

使用方式二行列转换的方式的好处：

- 灵活增加指标而不改动表结构

- 可以并行计算指标，运行效率高

- 字段输出顺序灵活可调

[1]: https://sdkfiledl.jiguang.cn/public/1a169f28d3cd4c1ab2927b18226052e7.gif
