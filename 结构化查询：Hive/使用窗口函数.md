## 窗口函数

如果存在多行输入，多行输出，并且要求输出的每行数据不仅包含它所在行的数据信息，还包括它所在分组内的其他行的信息，这时就需要窗口函数完成。

## 下单用户路径分析

![下单路径分析][1]

有时我们不仅想知道用户当前的行为，还想知道用户当前行为前后用户的行为轨迹：

从哪个页面着陆？            ------>   窗口内排序或偏移量

下单前的最后页面？          ------>   窗口内偏移量

下单之前浏览了多少页面？     ------>   窗口内聚合

## 窗口函数的语法

- 语法格式：

```
Function(arg1, ....argn) OVER ([PARTITION BY<...>] [ORDER BY<...>] [window_clause])
```

- 偏移量函数

    - First_value/Last_value

    - Lead/Lag

- 聚合函数

    - COUNT/SUM

    - AVG/MIN/MAX

- 排序函数

    - ROW_NUMBER()

    - RNAK()

    - DENSE_RANK()

## 实际操作

```sql
select
    user_id,
    session_id,
    landing_page,
    last_page,
    count_visit
from
(select
    user_id,
    session_id,
    active_name,
    first_value(req_url) over (partition by session_id order by time_tag asc) as landing_page,
    lag(req_url, 1) over (partition by session_id order by time_tag asc) as last_page,
    count(if(active_name = 'pageview', req_url, null)) over (partition by session_id order by time_tag asc rows between unbounded preceding and current row) as count_visit
from
    bigdata.weblog) t1
where 
    active_name = 'order'
```

这里的count_visit中添加了row between关键字用于指定窗口范围，默认是从开始到当前行。使用partition by进行分组，count_visit表示下单前浏览了多少个页面，last_page表示下单前浏览的最后一个页面，landing_page表示该用户从哪个页面开始浏览的。

运行结果：

```
hive> select
    >     user_id,
    >     session_id,
    >     landing_page,
    >     last_page,
    >     count_visit
    > from
    > (select
    >     user_id,
    >     session_id,
    >     active_name,
    >     first_value(req_url) over (partition by session_id order by time_tag asc) as landing_page,
    >     lag(req_url, 1) over (partition by session_id order by time_tag asc) as last_page,
    >     count(if(active_name = 'pageview', req_url, null)) over (partition by session_id order by time_tag asc rows between unbounded preceding and current row) as count_visit
    > from
    >     bigdata.weblog) t1
    > where
    >     active_name = 'order'
    > limit 5;
Query ID = 1015146591_20190106223135_9fbad1ed-4f61-41f0-bbe3-a64ea54c3234
Total jobs = 1
Launching Job 1 out of 1
Number of reduce tasks not specified. Estimated from input data size: 7
In order to change the average load for a reducer (in bytes):
  set hive.exec.reducers.bytes.per.reducer=<number>
In order to limit the maximum number of reducers:
  set hive.exec.reducers.max=<number>
In order to set a constant number of reducers:
  set mapreduce.job.reduces=<number>
Starting Job = job_1535253853575_21468, Tracking URL = http://bigdata0.novalocal:8088/proxy/application_1535253853575_21468/
Kill Command = /home/hadoop/hadoop-current/bin/hadoop job  -kill job_1535253853575_21468
Hadoop job information for Stage-1: number of mappers: 6; number of reducers: 7
2019-01-06 22:31:40,964 Stage-1 map = 0%,  reduce = 0%
2019-01-06 22:31:49,289 Stage-1 map = 17%,  reduce = 0%, Cumulative CPU 4.94 sec
2019-01-06 22:31:51,411 Stage-1 map = 18%,  reduce = 0%, Cumulative CPU 14.03 sec
2019-01-06 22:31:52,442 Stage-1 map = 20%,  reduce = 0%, Cumulative CPU 49.51 sec
2019-01-06 22:31:54,511 Stage-1 map = 31%,  reduce = 0%, Cumulative CPU 59.51 sec
2019-01-06 22:31:55,606 Stage-1 map = 40%,  reduce = 0%, Cumulative CPU 66.21 sec
2019-01-06 22:31:57,678 Stage-1 map = 48%,  reduce = 0%, Cumulative CPU 79.31 sec
2019-01-06 22:31:59,728 Stage-1 map = 48%,  reduce = 2%, Cumulative CPU 83.35 sec
2019-01-06 22:32:00,808 Stage-1 map = 64%,  reduce = 5%, Cumulative CPU 100.0 sec
2019-01-06 22:32:01,833 Stage-1 map = 70%,  reduce = 6%, Cumulative CPU 101.13 sec
2019-01-06 22:32:02,857 Stage-1 map = 80%,  reduce = 10%, Cumulative CPU 103.94 sec
2019-01-06 22:32:03,882 Stage-1 map = 100%,  reduce = 16%, Cumulative CPU 111.57 sec
2019-01-06 22:32:04,907 Stage-1 map = 100%,  reduce = 19%, Cumulative CPU 111.83 sec
2019-01-06 22:32:05,962 Stage-1 map = 100%,  reduce = 39%, Cumulative CPU 115.43 sec
2019-01-06 22:32:06,998 Stage-1 map = 100%,  reduce = 74%, Cumulative CPU 134.61 sec
2019-01-06 22:32:08,022 Stage-1 map = 100%,  reduce = 100%, Cumulative CPU 151.51 sec
MapReduce Total cumulative CPU time: 2 minutes 31 seconds 510 msec
Ended Job = job_1535253853575_21468
MapReduce Jobs Launched:
Stage-Stage-1: Map: 6  Reduce: 7   Cumulative CPU: 151.51 sec   HDFS Read: 1571477509 HDFS Write: 5090 SUCCESS
Total MapReduce CPU Time Spent: 2 minutes 31 seconds 510 msec
OK
9671531898173445        00022b2a858240b29d61daf4e2fa4e0f        http://www.bigdataclass.com/my/9671531898173445 http://www.bigdataclass.com/product/1527235438750267  11
5161531907295812        000dbaa1bff541719a5a49cbe014ce98        http://www.bigdataclass.com     http://www.bigdataclass.com/product/1527235438751118  16
4341531901218931        00269566c4a74e348392bf2c203afb19        http://www.bigdataclass.com/my/4341531901218931 http://www.bigdataclass.com/product/1527235438749603  9
8471531908135174        00277e11aec54eae8a3791a0d20b4aa0        http://www.bigdataclass.com/category    http://www.bigdataclass.com/product/1527235438751276  12
2301531907877448        002c4ca8e141433693187759f1f774f2        http://www.bigdataclass.com     http://www.bigdataclass.com/product/1527235438749743  11
Time taken: 33.993 seconds, Fetched: 5 row(s)
```

[1]: https://github.com/jiaoqiyuan/163-bigdate-note/raw/master/%E7%BB%93%E6%9E%84%E5%8C%96%E6%9F%A5%E8%AF%A2%EF%BC%9AHive/img/%E4%B8%8B%E5%8D%95%E7%94%A8%E6%88%B7%E8%B7%AF%E5%BE%84.png