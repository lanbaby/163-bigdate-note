## Linux下载

教程B站地址：[Linux安装](https://www.bilibili.com/video/av35028934/?p=2)。

使用centos6.10，下载地址：[centos6.10](https://mirrors.tuna.tsinghua.edu.cn/centos/6.10/isos/x86_64/CentOS-6.10-x86_64-minimal.iso)。

vmware自己在网上下载吧。

## 安装Linux虚拟机

使用iso镜像和vmware安装linux虚拟机。

安装过程中手动分区，分三个区，一个是 `/boot` 分区，一个是 `swap` 分区，这个两个分区都分200M大小就可以，剩下的一个分区给根分区 `/`。具体可以参考视频。

## 制作原始快照

配置一些初始信息，制作一个原始系统快照，通过克隆这个快照来快速创建四台虚拟机。

启动刚才安装的虚拟机，配置如下两个文件的信息：

```
/etc/sysconfig/network-scripts/ifcfg-eth0
DEVICE=eth0
# HWADDR=00:0C:29:95:87:DC
TYPE=Ethernet
ONBOOT=yes
NM_CONTROLLED=yes
BOOTPROTO=static
IPADDR=172.16.123.8
NETMASK=255.255.255.0
GATEWAY=172.16.123.2
DNS1=114.114.114.114
```

重启网卡：

```
service network restart
```

关闭防火墙：

```
chkconfig iptables stop
chkconfig iptables off
```

```
vi /etc/selinux/config
SELINUX=disabled
```

修改hosts文件，添加规划的四个节点ip：

```
vi /etc/hosts
172.16.123.11 node01
172.16.123.12 node02
172.16.123.13 node03
172.16.123.14 node04
```

删除网络文件 `/etc/udev/rules.d/70-persistent-net.fules`。

关闭虚拟机，拍摄快照。

## 配置克隆的node01...04

启动克隆的机器node01...04。

- 修改 `/etc/sysconfig/network-scripts/ifcfg-eth0` 中的ip地址：

```
IPADDR=172.16.123.11
```

- 修改 `/etc/sysconfig/network` 中的hostname，分别修改成对应的node01, node02, node03, node04:

```
HOSTNAME=node01
```

- 为了能让windows或者linux通过hostname访问到这几个虚拟机虚拟机，可以将这几个虚拟机的ip和hostname添加到windwos或linux的hosts文件中，具体添加方法自己百度。

- 关机拍快照。

- 重启机器。


## 搭建伪分布式环境

- 远程控制其他机器：

    ```
    ssh root@node01 'ls /'
    ```

- 设置免密登录：

    ```
    ssh-keygen -t dsa -P '' -f /root/.ssh/id_dsa
    cd ~/.ssh
    cat id_dsa.pub >> authorized_keys
    ```

    登录本机测试一下：

    ```
    ssh root@node01
    ```

- 安装JDK

    解压 jdk 和 hadoop 到 `/opt/sxt` 目录下，然后将 jdk 和 hadoop 路径添加到环境变量 `/etc/profile` 中：

    ```conf
    # JDK
    export JAVA_HOME=/opt/sxt/jdk1.8.0_181
    export PATH=$JAVA_HOME/bin:$PATH

    # Hadoop
    export HADOOP_HOME=/opt/sxt/hadoop-2.6.5
    export PATH=$HADOOP_HOME/bin:$HADOOP_HOME/sbin:$PATH
    ```

- 配置 hadoop

    1. 修改 `etc/hadoop/hadoop-env.sh` 、 `etc/hadoop/mapred-env.sh` 、 `etc/hadoop/yarn-env.sh` 的 `JAVA_HOME=` 内容，都改为:

        ```
        export JAVA_HOME=/opt/sxt/jdk1.8.0_181
        ```
    
    2. 修改 `etc/hadoop/core-site.xml` ，添加如下内容:

        ```xml
        <property>
            <name>fs.defaultFS</name>
            <value>hdfs://node01:9000</value>
        </property>

        <property>
            <name>hadoop.tmp.dir</name>
            <value>/var/sxt/hadoop/local</value>
        </property>
        ```

    3. 修改 `etc/hadoop/hdfs-site.xml` ，添加如下内容：

        ```xml
        <property>
            <name>dfs.replication</name>
            <value>1</value>
        </property>

        <property>
            <name>dfs.namenode.secondary.http-address</name>
            <value>node01:50090</value>
        </property>

        ```

    4. 修改 `etc/hadoop/slaves` ，添加当前主机的hostname：`node01`


    5. 启动 hdfs:

        ```
        start-dfs.sh
        ```

    6. 上传文件到 hdfs 时自定义块的大小：

        产生一个测试文件：

        ```
        for i in `seq 100000`; do echo "hello world $i" >> test.txt;done
        ```

        上传文件并指定块大小为1M=1048576Byte，这样的话大小为 1.9M 的 `test.txt` 文件就会放到两个块中存储：

        ```
        hdfs dfs -D dfs.blocksize=1048576 -put test.txt /user/root
        ```

        文件实际存储路径是 `/var/sxt/hadoop/local/dfs/data/current/BP-628645872-172.16.123.11-1553705247778/current/finalized/subdir0/subdir0`，当前目录下一共有四个文件，对应着两个块：

        ```
        blk_1073741825  blk_1073741825_1001.meta  blk_1073741826  blk_1073741826_1002.meta
        ```

        `blk_1073741825` 和 `blk_1073741825` 两个文件中存储的就是 `test.txt` 文件中的内容，hadoop是严格按照字节进行区分的，可以查看上面那两个文件结尾和开头的字符串信息就能发现，hadoop在进行文件文件处理时是考虑到这种一行切分到两个文件中的情形的，会进行特殊处理。

    7. 伪分布式搭建结束。

## 搭建完全分布式环境

1. 规划处4个节点，4个节点的功能分别为：

    |    | NN | SNN | DN |
    |:------:|:-:|:-:|:-:|
    | node01 | * |   |   |
    | node02 |   | * | * |
    | node03 |   |   | * |
    | node04 |   |   | * |

    node01 为 namenode 节点， node02/node03/node04 为 datanode 节点，同时 node02 作为 secondary namenode 节点。

2. 将 node01 上的 `/opt/sxt` 目录下的 hadoop 和 jdk 文件夹分别拷贝到 node02/node03/node04：

    ```
    scp -r /opt/sxt/ root@node02:/opt
    scp -r /opt/sxt/ root@node03:/opt
    scp -r /opt/sxt/ root@node04:/opt
    ```

3. 将 node02/node03/node04 中的 `/etc/profile` 文件中加入 jdk 和 hadoop 的执行命令路径：

    ```conf
    # JDK
    export JAVA_HOME=/opt/sxt/jdk1.8.0_181
    export PATH=$JAVA_HOME/bin:$PATH

    # Hadoop
    export HADOOP_HOME=/opt/sxt/hadoop-2.6.5
    export PATH=$HADOOP_HOME/bin:$HADOOP_HOME/sbin:$PATH
    ```

4. 免秘钥登录所有机器

    在 node02/node03/node04 上 `ssh localhosts` ，然后退出，以在 `/root` 目录下生成 `.ssh`目录。

    在 node01 上将 node01 的 `id_dsa.pub` 分别拷贝到 node02/node03/node04 的 `/root/.ssh/` 目录下：

    ```
    scp ~/.ssh/id_dsa.pub node02:~/.ssh/node01.pub
    scp ~/.ssh/id_dsa.pub node03:~/.ssh/node01.pub
    scp ~/.ssh/id_dsa.pub node04:~/.ssh/node01.pub
    ```

    分别在 node02/node03/node04 的 `/root/.ssh/` 目录下执行：

    ```
    cat node01.pub >> authorized_keys
    ```

    将 node01 的公钥文件添加到 node02/node03/node04 的 `authorized_keys` 文件中后即可实现 node01 对 node02/node03/node04 的免密访问。

5. 部署Hadoop

    修改 node01 的 `core-site.xml` :

    ```xml
    <property>
        <name>hadoop.tmp.dir</name>
        <value>/var/sxt/hadoop/full</value>
    </property>
    ```

    修改 node01 的 `slaves` 文件：

    ```
    node02
    node03
    node04
    ```

    修改node01 的 `hdfs-site.xml` 文件：

    ```xml
    <property>
        <name>dfs.replication</name>
        <value>2</value>
    </property>
    <property>
        <name>dfs.namenode.secondary.http-address</name>
        <value>node02:50090</value>
    </property>
    ```

    将 node01 节点上的 `/opt/sxt/hadoop-2.6.5` 文件夹拷贝到 node02/node03/node04 节点的 `/opt/sxt` 目录下：

    ```
    scp -r /opt/sxt/hadoop-2.6.5 node02:/opt/sxt/
    scp -r /opt/sxt/hadoop-2.6.5 node03:/opt/sxt/
    scp -r /opt/sxt/hadoop-2.6.5 node04:/opt/sxt/
    ```

6. 在 node01 开启分布式集群

    第一次启动先格式化：

    ```
    hdfs namenode -format
    ```

    格式化 namenode 成功后，启动集群：

    ```
    start-dfs.sh
    ```

7. 等待启动完毕，完全分布式Hadoop集群就搭建完成了。