## 插件使用

- hadoopJava

    是Java类型job的加强版，除了可以执行本地java任务类，还支持对Hadoop的操作，比如HDFS的访问，MR类型job的提交等等。

- Hive

    提供了访问Hive的能力，

- hadoopShell

    hadoopShell是Command的扩展，Command job的命令都是本地机器命令，hadoopShell还可以进行Hadoop相关Shell命令的使用

- hadoopSpark

    提供了调用Spark的能力。

- Command（内置）

    运行一条指令如 `echo hello` ，也可以用来调用脚本

- Noop（内置）

    什么都不做，是一个空跑的job，一般有如下作用：

    1. flow的命名，因为flow的名字是由job的最后一个DAG图的job的名字决定的，如果最后这个job只是Noop类型的话，flow的改名就会非常方便。假设最后一个job是实现的具体业务逻辑，那么当业务需要变更，删除该job的时候会非常麻烦，同时如果我们需要修改flow的名字时，也不得不修改业务job的名字，这很不合理。

    2. 实现job DAG的分支同步，job分支尾节点是同一个job的话，它们就可以实现这些job分支的同步归流，Noop类型的job是非常适合这个场景的，因为不会引入任何业务逻辑上的纠缠。

### 插件使用-hadoopJava

.job文件需要配置的参数如下：

- type:hadoopJava

- job.class 必填

- classpath 必填

- main.args 默认空

- dependencies 看具体情况

- user.to.proxy 推荐封掉

- Method.run 默认run

- Method.cancel 默认cancel

### 插件使用-Hive

.job文件需要配置的参数如下：

- type:hive

- hive.script 必填

- classpath 必填

- hiveconf.Foo 默认空

- hivevar.FOO 看具体情况

- Hadoop-inject.FOO 看具体情况

- user.to.proxy 推荐封掉


### 插件使用-spark

.job文件中需要配置的参数如下

- type:spark

- master=yarn

- deploy-mode=client

- driver-memory=1024m

- executor-memory=1024m

- language=java

- jars=lib/*

- class=azkaban.dataquality.hive.spark.HiveTableDataQuality

- execution-jar=lib/bi-dw-framework-1.0.jar

- conf.spark.dynamicAllocation.maxExecutors=1

### 插件使用-hadoopShell

.job文件需要配置的参数如下：

- type:Hadoopshell

- comand 必填

- dependencies 看具体情况

- Hadoop-inject.FOO

- user.to.proxy 推荐封掉

## Azkaban实战-构建DAG

- 项目打包成zip，一个项目中可以有多个Flow，一个Flow可以有多个job，Flow即DAG

- job之间通过dependencies参数控制依赖关系

- job之间不允许循环依赖

- 同一个job可以在多个flow中出现

- flow的名字有DAG的最后一个job名字决定

- 公共的job参数可以提取到common.properties中

- job文件是working.dir，资源的引用必须以相对的working.dir来处理。