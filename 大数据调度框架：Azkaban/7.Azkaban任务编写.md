## 概述

常用的Azkaban支持的plugin类型有以下这些

    command：Linux shell命令任务
    hadoopJava：运行HadoopMR任务
    java：原生java任务
    hive：支持执行hiveSQL
    pig：pig脚本任务
    spark：spark任务

## 编写一个command的job过程如下：

1. 创建 cmd1.job 文件，内容如下：

    ```conf
    type=command
    command=echo "This is azkaban command job"
    ```

2. 创建 cmd2.job 文件，内容如下：

    ```conf
    type=command
    command=echo "This is azkaban command job"
    dependencies=cmd1
    ```

    可以看出job间是通过dependencies属性进行依赖的

3. 打包zip文件

    azkaban要求压缩包解压后就可以看到job文件，而不是包含了一层目录，而且要打包成zip文件

4. 常用方式

    如果使用command类型，效果其实跟在本地执行linux shell命令一样，一般情况下是通过command命令调用python等脚本文件唤起具体的操作，而不是直接将命令写在command参数里面

## 编写一个java类型的过程如下

1. 代码编写 `AzkabanJavaJob.java`:

    ```java
    package com.bigdata.java;

    public class AzkabanJavaJob {
        public static void main(String[] args) {
            System.out.println("Hello World");
        }
    }
    ```

2. 打包成jar文件：使用maven或者打包工具导出为jar文件

3. 编写job文件 `java.job`

    ```conf
    type=javaprocess
    classpath=./lib/*,${azkaban.home}/lib/*
    java.class=com.bigdata.java.AzkabanJavaJob
    ```

4. 组成一个完整的运行包，将打包的jar和job文件放到同一级目录下

5. 打包成zip文件，把jar包和job文件打包成zip文件

6. 提交运行

## 编写一个hadoopJava类型的任务（打包方式类似java任务）

1. 准备课程中输入的数据和mr代码程序

2. 编写job文件 `bigdata.job` 文件内容如下：

    ```conf
    type=hadoopJava
    job.extend=false
    job.class=com.bigdata.java.xxx
    classpath=./lib/*,${azkaban.home}/lib/*
    force.output.overwrite=true
    input.path=xxx
    output.path=xxx
    ```

3. 打包提交运行

## 编写一个Hive类型的任务

1. 创建一个 `hive.sql` 文件：

    ```sql
    use hadoop;

    insert overwrite table 
        member_1 partition (day='2018-12-28')
    select userid, name, sex, age
    from member
    ```

    编写完成后，把文件放入文件夹中。

2. 编写 `hive.job` 文件：

    ```conf
    type=hive
    user.to.proxy=azkaban
    classpath=./lib/*,${azkaban.home}/lib/*
    azk.hive.action=execute.query
    hive.script=文件路径/hive.sql
    ```

3. 打包

4. 提交运行

## 编写一个spark类型的过程如下

spark任务有两种运行方式，一种是command类型 ，一种是spark类型

1. 准备好spark任务的代码

    ```java
    package com.bigdata.sparkjob

    import org.apache.spark.{SparkConf, SparkContext}
    import org.apache.spark.sql.{SQLContext}

    object WordCount {
        def main(args: Array[String]) {
            val sc = new SparkContext(new SparkConf().setAppName("WordCount"))
            val file = spark.sparkContext.textFile(args(0))
            val wordCounts = file.flatMap(line => line.splite(" ")).map(word => (word, 1)).reduceByKey(_+_)
            //数据collect，到driver端打印
            wordCounts.collect().foreach(println _)
        }
    }
    ```

2. 打包jar包

3. spark job 编写

    command 命令提交方式：

    ```conf
    type=command
    command=${spark.home}/bin/spark-submit --master yarn-cluster --class com.bigdata.sparkjob.WordCount jar包名 参数列表
    ```

    spark类型

    ```conf
    type=spark
    master=yarn-cluster
    execution-jar=lib/spark-template-1.0-SNAPSHOT.jar
    class=com.bigdata.sparkjob.WordCount
    参数列表
    ```

4. 打包zip，提交运行

