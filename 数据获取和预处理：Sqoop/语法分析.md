## 数据导入

以下面这个sqoop语法为例：

```
./sqoop import --connect "jdbc:mysql://*.*.*.*:protNum/DbName?characterEncoding=UTF-8&useCursorFetch=true" --username *** --password *** --query "SELECT * FROM HIVE_JDBC_TEST2 WHERE id > 2 AND \$CONDITIONS" --split-by id --target-dir "/user/hadoop/HIVE_JDBC_TEST2_QUERY" --delete-target-dir --as-textfile
```

参数介绍：

- --connect：指定JDBC连接数据库所在的地址、端口、数据库名称、字符编码格式等信息

- --username：数据库连接用户名

- --password：数据库连接密码

- --query：获取导入数据的查询语句

- --split-by：指定分片字段，后续会使用改字段对数据进行换分，放到不同的map中执行，一般使用主键或者有索引的字段，可提高执行效率

- --target-dir：指定HDFS上的目标路径，即导入数据的存储路径

- --delete-target-dir：如果目标文件存在就先删除在导入

- --as-textfile：指定导入数据到HDFS后的数据存储格式。

    - 支持有text、sequence、avro、parquet

- --boudary-query：指定数据边界查找语句。sqoop默认使用select min(), max() from 来查找split的总边界。但是这种自动生成的查询语句执行效率并不高。

- -m/--num-mappers指定Map个数。这个参数会显著影响程序的执行效率，太小的话并发数不够，程序执行效率低，太大的话任务的启动开销过大，任务执行也会受影响。

- --table,column,where分别对应于Table，Column，Where限制条件。

- --compress，--compression-codec：使用压缩

- --map-column-java：指定Column在Sqoop对象中的数据类型，一般不需要手动添加。

- 增量导入（incremental）：指定关系数据库的某个Column和last value，下次导入的时候只导入比last value大的数据。增量导入分两种模式：

    - append：把新数据追加到目标路径中

    - last modified：会将新数据和老数据进行合并，合并的话需要新老数据的对应关系，所以还需要添加参数merge key来指定主键，最后执行结果只保留最新数据。该模式下last value的值一定要是timestamp或者data

## Hive Import

实际使用中的具体命令如下：

```
./sqoop import --connect "jdbc:mysql://*.*.*.*:protNum/DbName?characterEncoding=UTF-8&useCursorFetch=true" --username *** --password *** --table sqooptest --split-by id --target-dir "/user/hadoop/sqooptest" --delete-target-dir --hive-import --hive-table sqooptest map-column-hive id=INT,name=STRING
```

- 支持hive table的创建与覆盖：create-hive-table, hive-overwritess

- 支持hive分区：hive-partition-key,hive-partition-value

- map-column-hive:指定hive column的数据类型。例如key=value。限制：Hive Column顺序需等同与sqoop导入的hdfs文件column顺序，否则会导入失败（可在-column或者-query中指定column顺序）

- 支持指定查询语句，解决多表关联插入。

```
./sqoop import --connect "jdbc:mysql://10.173.32.6:3306/sqoop?characterEncoding=UTF-8&useCursorFetch=true" --username root --password Gg/ru,.#5 --table sqoop_test --delete-target-dir --target-dir /user/1015146591/sqoop_test --split-by id
```

## 数据导出

```
sqoop export --connet jdbc:mysql://10.173.121.103:3306/test?characterEncoding=UTF8 --username mammoth_test --password VwvV8r37ccrR --table mysql_base_export -columns id,name --export-dir /user/hadoop/mysql_base_no_meta
```

- table, colums:指定导出rdms中的table，columns

- 支持sequence，avro，parquet导出。其中sequence，avro，parquet格式导出sqoop可自动识别

- update-mode:分为updateonly(默认)和allowinsert两种类型。allowinsert可进行update insert操作

- update-key:指定根据指定列完成update。

## RDMS工具

在导入导出数据时，常常需要对数据库中的数据进行探测，看看数据库中表的结构是什么。sqoop内部提供了查询工具。

```
sqoop list-databases --connect jdbc:mysql://*.*.*.*:3306/dbName?characterEncoding-UTF-8 --username *** --password ***
```
rdms结构：

- list-databases:列出所有数据库

- list-tables: 列出所有的表

查看数据表内数据：

```
sqoop eval --connect jdbc:mysql://*.*.*.*:3306/dbName?characterEncoding-UTF-8 --username *** --password *** -e "select * from ..."
```
- -e,-query:添加要执行的SQL查询语句