## 案例

假设有这样一张表：

| order | order_id | item | user | description | create_time |
|:-----:|:--------:|:----:|:----:|:-----------:|:-----------:|
| 1 | 0001 | hive课程 | 李磊 | 1学时 | 15833081600 |
| 2 | 0002 | spark课程 | 韩梅 | 1学时 | 15833081600 |

1. 快照导入：

    ```
    ./sqoop import --connect "jdbc:mysql://*.*.*.*:protNum/DbName?characterEncoding=UTF-8" --username *** --password *** --table order --split-by order_id --target-dir "/user/hadoop/order_snapshot/2018-08-01" --delete-target-dir --as-parquet
    ```

2. 增量导入，每天只导入增量的数据：

    ```
    ./sqoop import --connect "jdbc:mysql://*.*.*.*:protNum/DbName?characterEncoding=UTF-8" --username *** --password *** --query "SELECT * FROM order WHERE create_time > 1533081600 and \$CONDITIONS" --split-by order_id --target-dir "/user/hadoop/order_snapshot/2018-08-01" --as-parquet
    ```

    $CONDITIONS在这里是起到占位符的作用，在任务执行过程中会被数据划分生成的条件替代，一定要注意添加。

选择全量导入还是增量导入需根据ETL规范视情况而定。

## 数据导出

数据导入HDFS后，业务程序关联各种数据后可能每天会生成一个收入报表：

| order | catalog | income | order_num | description | create_time |
|:-----:|:--------:|:----:|:----:|:-----------:|:-----------:|
| 1 | hive课程 | 100 | 1000 | 1学时 | 15833081600 |
| 2 | spark课程 | 100 | 1000 | 1学时 | 15833081600 |

对于数据量较小的访问，HDFS的效率并不高，所以此时可以将每天的收入报表从HDFS导出到关系型数据库中来进行分析。

导出任务

```
./sqoop export --connect "jdbc:mysql://*.*.*.*:protNum/DbName?characterEncoding=UTF-8" --username *** --password *** --table income_report --columns catalog,income,order_num,description,create_time --export-dir "/user/hadoop/order_report/"
```

--export-dir:表示待导出数据在HDFS上的路径。

## 总结

- 为什么需要sqoop，是由于数据同步的需求。

- sqoop是什么，sqoop是一个基于MapReduce框架的数据同步工具

- Sqoop是怎么工作的。

- sqoop的使用方法。